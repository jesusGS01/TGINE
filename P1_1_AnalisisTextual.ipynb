{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S60jurJC7vux"
      },
      "source": [
        "# Sesión 1. Análisis textual en Python\n",
        "\n",
        "En esta sesión se pretende trabajar con algunos de los conceptos básicos de Python para el procesamiento de texto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apartado 1.0\n",
        "\n",
        "Descargamos primero el dataset \"datasetEspañol.csv\" con el que vamos a trabajar."
      ],
      "metadata": {
        "id": "DsZ5t86ADVBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dis.um.es/~valencia/recursosTGINE/datasetEspañol.csv"
      ],
      "metadata": {
        "id": "WwF8zpifECU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apartado 1.1\n",
        "Para ello cargaremos primero el dataset en CSV proporcionado \"datasetEspañol.csv\" usando la librería **pandas**\n",
        "\n",
        "Mostraremos también las primeras líneas del CSV cargado"
      ],
      "metadata": {
        "id": "7WsY3jNWDTSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6LcR5vg8FyP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lplAkxQn8iSB"
      },
      "source": [
        "## Apartado 1.2\n",
        "\n",
        "Seleccionamos únicamente las 200 primeras filas y las columnas 'twitter_id', 'twitter_created_at', 'tweet', 'user' y 'label' y guardamos de nuevo el CSV en el fichero \"datasetEspañolReducido.csv\".\n",
        "\n",
        "A partir de ahora trabajaremos con este dataset reducido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4ZsZNUI8hsZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PX82ZoM-CpS"
      },
      "source": [
        "## Apartado 1.3\n",
        "\n",
        "Ahora trabajaremos detectando de manera sencilla algunas expresiones regulares usando la librería **re**.\n",
        "\n",
        "Para ello seleccionaremos los **hashtags** y **menciones** de los tuits.\n",
        "\n",
        "Una expresión regular para detectar los hashtags podría ser la siguiente:\n",
        "\\#[A-Za-záéíóúÁÉÍÓÚÜüÑñ0-9\\_\\-]+\n",
        "\n",
        "Además, crearemos una nueva columna 'tweet_clean' que no contenga los hashtags ni menciones.\n",
        "\n",
        "- Usaremos la función \"apply\" y \"lambda\" de Pandas.\n",
        "- Para detectar si la expresión regular existe en un determinado String usaremos la función re.sub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNH0DuBb8bXJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3-1P1nG-Ufx"
      },
      "source": [
        "## Apartado 1.4\n",
        "\n",
        "Una vez detectadas las expresiones regulares, procederemos a crear dos nuevas columnas con los **hashtags** y **menciones** respectivamente.\n",
        "\n",
        "- Podemos usar la función re.findall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBUuMYx77vWJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwknkL7Q_GYq"
      },
      "source": [
        "## Apartado 1.5\n",
        "\n",
        "Sobre esa nueva columna 'tweet_clean' quitaremos los símbolos de puntuación haciendo uso de la librería **string**\n",
        "\n",
        "Podemos usar la siguiente función\n",
        "\n",
        "```\n",
        "#defining the function to remove punctuation\n",
        "import string\n",
        "\n",
        "spanish_punctuation = string.punctuation+'¿'+'¡'\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in spanish_punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uuEB1o57sD6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU5mvUWaBayq"
      },
      "source": [
        "## Apartado 1.6\n",
        "\n",
        "Cambiamos el texto de la columna 'tweet_clean' y lo podemos todo en *lowercase*.\n",
        "\n",
        "Para eso utilizamos la función lower() del objeto string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz5gY15wBhxb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJFOXqc7BiU0"
      },
      "source": [
        "## Apartado 1.7\n",
        "\n",
        "Aplicamos un tokenizer sencillo y guardamos todos los tokens de los tuits limpios en otra columna 'tweet_clean_tokens' usando la siguiente función sencilla de Tokenizer.\n",
        "\n",
        "```\n",
        "#defining function for tokenization\n",
        "import re\n",
        "def tokenization(text):\n",
        "    tokens = re.split('\\W+',text)\n",
        "    return tokens\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khZwoUieB-LS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzB2bCX0B_GL"
      },
      "source": [
        "## Apartado 1.8\n",
        "\n",
        "**NLTK** es una librería con distintas herramientas para el PLN. La vamos a utilizar para descargar las stopwords en español y para usar su stemmer.\n",
        "\n",
        "El siguiente paso sería eliminar las stopwords de los tokens usando la librería **NLTK**. Ver función siguiente.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import nltk\n",
        "#Stop words present in the library\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "    \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2i30XB0CyWr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiizdFBuCzHS"
      },
      "source": [
        "## Apartado 1.9\n",
        "\n",
        "Por último usando el SnowballStemmer de NLTK obtenemos los stems de cada una de los tokens sin las stopwords y lo guardamos en otra columna 'tweet_clean_stemmed_tokens'\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "  stem_text = [stemmer.stem(word) for word in text]\n",
        "  return stem_text\n",
        "  ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLcci954DIgX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apartado 1.10  Simple corrección ortográfica (Resuelto)\n",
        "Muchos textos tienen errores léxicos y hay distintas librerías para la corrección ortográfica a partir de diccionarios. Una de ellas es la librería **pyspellchecker**\n",
        "\n",
        "Hay otras opciones como hunspell y pyenchant que hacen una corrección léxica basada en diccionarios"
      ],
      "metadata": {
        "id": "KmZxuYX0Z4_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalamos la libería\n",
        "!pip3 install pyspellchecker\n",
        "\n",
        "#importamos la librería\n",
        "import spellchecker\n",
        "\n",
        "texto_erróneo = \"La asginatura del master haze trabajar y aprehnder procesamiengo de teexto\"\n",
        "\n",
        "# Crea un objeto SpellChecker para el idioma especificado\n",
        "spell = spellchecker.SpellChecker(language='es')\n",
        "\n",
        "# Divide el texto en palabras\n",
        "palabras = texto_erróneo.split()\n",
        "\n",
        "# Inicializa una lista para las palabras corregidas\n",
        "palabras_corregidas = []\n",
        "\n",
        "# Verifica cada palabra en el texto\n",
        "for palabra in palabras:\n",
        "# Si la palabra está mal escrita, sugiere correcciones\n",
        "   correccion = spell.correction(palabra)\n",
        "   palabras_corregidas.append(correccion)\n",
        "\n",
        "# Unimos las palabras corregidas para formar el texto corregido\n",
        "texto_corregido = ' '.join(palabras_corregidas)\n",
        "print(texto_corregido)"
      ],
      "metadata": {
        "id": "TtSpbXBWZ4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeECKZyHDKtC"
      },
      "source": [
        "## Ejercicio a resolver y entregar\n",
        "Una vez visto el framework stanza en el siguiente Notebook P1.2, crear una columna 'tweet_entities' con las entidades del texto.\n",
        "\n",
        "Una mejora de este ejercicio es crear una columna para cada tipo de entidad detectada. Esto es necesario para tener la máxima nota en el ejercicio\n",
        "\n",
        "Debido a que puede tardar bastante tiempo, podéis hacerlo con un subconjunto del dataset de unas 20 líneas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bIix4seDBg7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}