{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusGS01/TGINE/blob/main/3_1_scrapyRSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Sesión 3 - Scrapy\n",
        "En esta sesión se va a ver una breve introducción a cómo se pueden crear Crawlers sencillos con la librería Scrapy de cara a hacer la práctica 1.\n",
        "\n",
        "Lo primero que haremos será instalarnos la librería"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-5KvKJ09JU-"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de ficheros RSS\n",
        "Para desarrollar un Crawler en Scrapy debemos definir una clase Spider que herede de scrapy.Spider. Cada clase definirá los métodos necesarios para realizar el scrapping del sitio web y cómo obtener información estructurada.\n",
        "\n",
        "Para poder crear Spiders adecuados es NECESARIO conocerse los selectores CSS de HTML.\n",
        "https://www.w3schools.com/cssref/css_selectors.asp\n",
        "\n",
        "En el ejemplo de abajo está comentado cómo se haría el scrapping the un fichero en RSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhF7pBm-vHNR"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import string\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class RSSSpider (scrapy.Spider):\n",
        "\n",
        "    # Es obligatorio poner el nombre del Spider\n",
        "    name = 'RSS'\n",
        "\n",
        "    # Estas son las URLs donde empieza a buscar y se podrían poner varias distintas de RSS feeds\n",
        "    start_urls = ['https://www.abc.es/rss/2.0/portada/',\n",
        "                  'https://www.laverdad.es/rss/2.0/portada',\n",
        "                  'https://www.europapress.es/rss/rss.aspx',\n",
        "                  'https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "\n",
        "    def parse (self, response):\n",
        "        \"\"\"\n",
        "        @inherit\n",
        "\n",
        "        @param self\n",
        "        @param response\n",
        "        \"\"\"\n",
        "\n",
        "        #Buscamos todos los elementos en el archivo XML con la etiqueta <item>\n",
        "        for item in response.css ('XXXXXX'):\n",
        "            #Obtenemos por cada elemento <item> el texto del subelemento <link>\n",
        "            url = str (item.css ('XXXXXX').get ()).strip()\n",
        "            #Obtenemos por cada elemento <item> el texto del subelemento <title>. Además co el BeautifulSoup\n",
        "            #procesamos el texto en html y nos quedamos con el texto\n",
        "            title = BeautifulSoup(str(item.css ('XXXXXXXX').get()), 'html.parser').get_text().strip()\n",
        "            #Obtenemos por cada elemento <item> el texto del subelemento <title>\n",
        "            content = BeautifulSoup(str(item.css ('XXXXXXXXXX').get()), 'html.parser').get_text().strip()\n",
        "\n",
        "            #Imprimimos la información obtenida para comprobar lo que estamos extrayendo\n",
        "            print (\"-------------------------\")\n",
        "            print ('URL:' + url)\n",
        "            print ('Título:' + title)\n",
        "            print ('Descripción:' + content)\n",
        "            print (\"-------------------------\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'content': content\n",
        "            }\n",
        "\n",
        "            #Creamos para cada item un fichero json y para ello obtenemos un número aleatorio.\n",
        "            filename = str(random.random()).replace(\".\",\"\") + \".json\"\n",
        "\n",
        "            # Si tenemos descripción, url y título entonces lo guardamos a disco en la carpeta 'rss'\n",
        "            if content and title and url:\n",
        "                with open ('rss/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2 - Lanzar el Crawler RSSSpyder\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H27QM4rJCapR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('rss')):\n",
        "    os.mkdir('rss')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(RSSSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}