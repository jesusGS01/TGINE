{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusGS01/TGINE/blob/main/2_1_Gate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_-ImClQ1_Ot"
      },
      "source": [
        "# Sesión 2 - Python GATEnlp\n",
        "\n",
        "Python GATEnlp es una herramienta gráfica que permite definir distintos pipelines para la obtención de anotaciones a través del uso de Gazzetteers y reglas.\n",
        "\n",
        "El objetivo de la práctica es ver las posibilidades de GATE y crear distintos recursos para realizar la detección de entidades usando un enfoque basado en conocimiento. Iremos procesando un texto de ejemplo.\n",
        "\n",
        "Lo primero que haremos será instalar GATEnlp y stanza y descargar el modelo en español de stanza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jQ1tLgBaG1z"
      },
      "outputs": [],
      "source": [
        "#quitamos que se muestren los mensajes de log como DEBUG e INFO\n",
        "import logging, sys\n",
        "logging.disable(sys.maxsize)\n",
        "\n",
        "\n",
        "!pip3 install gatenlp[all]\n",
        "\n",
        "!pip3 install stanza\n",
        "import stanza\n",
        "stanza.download('es')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYHxHThPanB9"
      },
      "source": [
        "## Apartado 1.1 (Resuelto)\n",
        "\n",
        "Cargamos un documento en inglés y lo anotamos con el servicio de GateCloudAnnotator (https://cloud.gate.ac.uk/) y más concretamente el de ANNIE. Este componente permite la detección de distintas entidades como son:\n",
        "*   :Person\n",
        "* :Location\n",
        "* :Organization\n",
        "* :Date\n",
        "* :Address\n",
        "* :Money\n",
        "* :Percent\n",
        "* :Token\n",
        "* :SpaceToken\n",
        "* :Sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3oxzxIbamKQ"
      },
      "outputs": [],
      "source": [
        "# Definimos un ejemplo de texto en inglés.\n",
        "texto_en= \"\"\"\n",
        "Roger Federer (German: [ˈrɔdʒər ˈfeːdərər]; born 8 August 1981) is a Swiss professional tennis player. He is ranked No. 9 in the world by the Association of Tennis Professionals (ATP). He has won 20 Grand Slam men's singles titles, an all-time record shared with Rafael Nadal and Novak Djokovic. Federer has been world No. 1 in the ATP rankings a total of 310 weeks – including a record 237 consecutive weeks – and has finished as the year-end No. 1 five times. Federer has won 103 ATP singles titles, the second most of all-time behind Jimmy Connors, including a record six ATP Finals.\n",
        "\n",
        "Federer has played in an era where he dominated men's tennis together with Rafael Nadal and Novak Djokovic, who have been collectively referred to as the Big Three and are widely considered three of the greatest tennis players of all-time.[c] A Wimbledon junior champion in 1998, Federer won his first Grand Slam singles title at Wimbledon in 2003 at age 21. In 2004, he won three out of the four major singles titles and the ATP Finals,[d] a feat he repeated in 2006 and 2007. From 2005 to 2010, Federer made 18 out of 19 major singles finals. During this span, he won his fifth consecutive titles at both Wimbledon and the US Open. He completed the career Grand Slam at the 2009 French Open after three previous runner-ups to Nadal, his main rival up until 2010. At age 27, he also surpassed Pete Sampras's then-record of 14 Grand Slam men's singles titles at Wimbledon in 2009.\n",
        "\n",
        "Although Federer remained in the top 3 through most of the 2010s, the success of Djokovic and Nadal in particular ended his dominance over grass and hard courts. From mid-2010 through the end of 2016, he only won one major title. During this period, Federer and Stan Wawrinka led the Switzerland Davis Cup team to their first title in 2014, adding to the gold medal they won together in doubles at the 2008 Beijing Olympics. Federer also has a silver medal in singles from the 2012 London Olympics, where he finished runner-up to Andy Murray. After taking half a year off in late 2016 to recover from knee surgery, Federer had a renaissance at the majors. He won three more Grand Slam singles titles over the next two years, including the 2017 Australian Open over Nadal and a men's singles record eighth Wimbledon title later in 2017. He also became the oldest ATP world No. 1 in 2018 at age 36.\n",
        "\n",
        "A versatile all-court player, Federer's perceived effortlessness has made him highly popular among tennis fans. Originally lacking self-control as a junior, Federer transformed his on-court demeanor to become well-liked for his general graciousness, winning the Stefan Edberg Sportsmanship Award 13 times. He has also won the Laureus World Sportsman of the Year award a record five times. Outside of competing, he played an instrumental role in the creation of the Laver Cup team competition. Federer is also an active philanthropist. He established the Roger Federer Foundation, which targets impoverished children in southern Africa, and has raised funds in part through the Match for Africa exhibition series. Federer is routinely one of the top ten highest-paid athletes in any sport and ranked first among all athletes with $100 million in endorsement income in 2020.\"\"\"\n",
        "\n",
        "from gatenlp import Document\n",
        "from gatenlp.processing.client.gatecloud import GateCloudAnnotator\n",
        "\n",
        "# Definimos el anotador en la nube\n",
        "annotator = GateCloudAnnotator(\n",
        "    url=\"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\",\n",
        "    outset_name=\"ANNIE\",\n",
        "    ann_types=\":Address,:Date,:Location,:Organization,:Person,:Money,:Percent,:Token,:SpaceToken,:Sentence\"\n",
        ")\n",
        "\n",
        "doc = Document(texto_en)\n",
        "# Ejecutamos el annotador y mostramos el documento anotado\n",
        "doc = annotator(doc)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82rH1prP4xlC"
      },
      "source": [
        "## Apartado 1.2 (Resuelto)\n",
        "\n",
        "Vamos a hacer un ejemplo de reglas para la detección de entidades usando los Gazetteers que son un conjunto de listas de palabras que se identificarán en GATE. Para ello descargamos el ejemplo que se proporciona y está en el AulaVirtual y se descomprime.\n",
        "\n",
        "Cargamos un texto de ejemplo que se proporciona en español y se muestra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQJ-yvFNnfMh"
      },
      "outputs": [],
      "source": [
        "#Descargamos los ficheros de ejemplo\n",
        "!wget -q http://dis.um.es/~valencia/recursosTGINE/gatenlpUM.zip -O gatenlpUM.zip\n",
        "!unzip -o gatenlpUM.zip\n",
        "\n",
        "import os\n",
        "from gatenlp import Document\n",
        "from gatenlp.processing.gazetteer import TokenGazetteer, StringGazetteer\n",
        "from gatenlp.processing.tokenizer import NLTKTokenizer\n",
        "\n",
        "# Cargamos un documento a partir de un fichero y lo mmostramos\n",
        "doc = Document.load(\"rafa_nadal.txt\")\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf0AylVu_ma4"
      },
      "source": [
        "## Apartado 1.3 (Resuelto)\n",
        "\n",
        "Para poder usar los módulos de Gazetteer y PAMPAC de GATE es necesario tener un Tokenizer definido. Nosotros vamos a usar el Tokenizer de Stanza. No solamente utilizaremos el Tokenizer sino que también obtendremos las categorías gramaticales haciendo uso del POS Tagger y también utilizaremos la detección de entidades. Para esto se define una función llamada *obtanerAnotacionesStanzaEnGate* que se describe a continuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQp-tkQAkiMw"
      },
      "outputs": [],
      "source": [
        "# definimos una función para crear las anotaciones de Stanza para que se muestren en GATE\n",
        "def obtenerAnotacionesStanzaEnGate (doc):\n",
        "  import string\n",
        "  spanish_punctuation = string.punctuation + '¿'+'¡'\n",
        "\n",
        "  nlp = stanza.Pipeline(lang='es', processors='tokenize,pos,ner')\n",
        "  doctext = nlp(doc.text)\n",
        "\n",
        "  annset = doc.annset()\n",
        "  for sent in doctext.sentences:\n",
        "    for tok in sent.tokens:\n",
        "      kind = \"word\"\n",
        "      orth = \"lowercase\"\n",
        "      if tok.text.isupper():\n",
        "        orth = \"uppercase\"\n",
        "      elif tok.text[0].isupper():\n",
        "        orth = \"upperInitial\"\n",
        "      if tok.text.isnumeric():\n",
        "        kind = \"number\"\n",
        "      elif tok.text in spanish_punctuation:\n",
        "        kind = \"punctuation\"\n",
        "      ann = annset.add(tok.start_char,tok.end_char,\"Token\",{'string':tok.text, 'kind':kind, 'orth':orth, 'length':tok.end_char-tok.start_char, 'pos': tok.words[0].upos if tok.words[0].upos else \"\"})\n",
        "  for ent in doctext.ents:\n",
        "      ann = annset.add(ent.start_char,ent.end_char, ent.type,{'string': ent.text})\n",
        "  return doc\n",
        "\n",
        "#limpiamos las anotaciones\n",
        "doc.annset().clear()\n",
        "#ejecutamos la función de anotación de Stanza\n",
        "doc=obtenerAnotacionesStanzaEnGate(doc)\n",
        "#mostramos el documento\n",
        "doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzLNQe6ALFI"
      },
      "source": [
        "## Apartado 1.4 - Gazetteer (Resuelto)\n",
        "\n",
        "Los Gazetteers son listas de expresiones de texto que representan algo como pueden ser nombres de ciudades, nombres de primera persona, meses del año, etc. Todas estas listas se definen en fichero list.def. Este fichero es un índice con el siguiente formato:\n",
        "loc_spanish_city.lst:location:city\n",
        "spanish_firstname.lst:person_first\n",
        "\n",
        "```\n",
        "loc_spanish_city.lst:location:city\n",
        "spanish_firstname.lst:person_first\n",
        "```\n",
        "En la primera columna se define el nombre del fichero que contiene la lista y seguidamente se define el **majorType** y el **minorType**.\n",
        "\n",
        "Las anotaciones resultantes del proceso se suelen guardar en el tipo de anotación **Lookup**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0_zFx0fbTtx"
      },
      "outputs": [],
      "source": [
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO61qWWXF2j0"
      },
      "source": [
        "## Apartado 1.5 (Resuelto)\n",
        "\n",
        "PAMPAC “PAttern Matching with PArser Combinators” permite definir reglas complejas para la anotación de entidades en el texto a partir de patrones de texto.\n",
        "\n",
        "Para eso se definen un conjunto de reglas que se basan en un tipo de expresiones regulares. Por ejemplo, la siguiente regla obtendrá todas las anotaciones de tipo **Lookup** cuyo majorType sea *\"location\"* y creará una nueva anotación llamada **LOC**.\n",
        "\n",
        "```\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNkAJ2ZqeQrv"
      },
      "outputs": [],
      "source": [
        "from gatenlp.pam.pampac import Ann, AnnAt, Rule, Pampac, AddAnn, N, Seq, Or\n",
        "from gatenlp.pam.matcher import FeatureMatcher\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFXG6Tr0KWEP"
      },
      "source": [
        "## Apartado 1.6\n",
        "Vamos a crear un conjunto de reglas para identificar nombres de persona en español:\n",
        "* Creamos un nuevo gazetteer “surname.lst”.\n",
        "* Insertamos “Nadal”, \"Parera\", \"Djokovic\" y “Ferrer” en la lista.\n",
        "* Insertamos una nueva línea en el fichero list.def:\n",
        " * surname.lst:surname\n",
        "\n",
        "Creamos nuevas reglas para identificar personas (**PER**):\n",
        "* Regla 2:\n",
        "  * Una persona se forma por un “person_first” y un “surname”\n",
        "\n",
        "```\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "      AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "      AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "      name=\"person1\"\n",
        "      ),\n",
        "    # then the action for the patter\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "```\n",
        "\n",
        "* Regla 3:\n",
        "  * Una persona se forma por un “person_first” y un *Token* con su primer caracter en *uppercase*\n",
        "\n",
        "* Regla 4:\n",
        "  * Una persona se forma por un *Token* con su primer caracter en *uppercase* y un  *“surname”*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2W3zmdJHVl-"
      },
      "outputs": [],
      "source": [
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "        name=\"person1\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "\n",
        "#Crear nueva regla r4\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7QlZq2iXDvL"
      },
      "source": [
        "## Apartado 1.7\n",
        "Modificamos la regla 3 para indicar que una **PER** está formada por 1 o 2 *“person_first”*\n",
        "\n",
        "```\n",
        "Rule 3:\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "```\n",
        "\n",
        "Modificamos la regla 4 indicando que una **PER** está formada por 1 o 2 *“surname”*\n",
        "\n",
        "\n",
        "```\n",
        "Rule 4:\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyOVO1WFXSIb"
      },
      "outputs": [],
      "source": [
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(N(AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),min=1, max=2), N(AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),min=1,max=2), name=\"person1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r4\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2, r3, r4]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBilOcY9Ye0C"
      },
      "source": [
        "## Ejercicio 1\n",
        "Crear los recursos necesarios para identificar fechas en español con los siguientes patrones (reglas):\n",
        "\n",
        "* Number + “de” +Month + “de” + Number\n",
        "   * 12 de agosto de 2006\n",
        "* Number\n",
        "   * 2008\n",
        "* Month + \"de\" + Number\n",
        "  * diciembre de 2023\n",
        "\n",
        "Cambiar la primera regla para que pueda identificar lo siguiente\n",
        "* [Day] + Number + “de” + month + [\"de\" + Number]\n",
        "   * Lunes 15 de marzo, martes 12 de junio de 2023, 12 de junio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK1ARnfwXbnT"
      },
      "outputs": [],
      "source": [
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(N(AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),min=1, max=2), N(AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),min=1,max=2), name=\"person1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r4\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2, r3, r4]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg7FHFeEYjeK"
      },
      "source": [
        "## Ejercicio 2 - Para entregar\n",
        "\n",
        "Crear los recursos necesarios para identificar localizaciones en español con los siguientes patrones (reglas):\n",
        "\n",
        "* “en” + Location\n",
        "  * en Murcia, en Orizaba\n",
        "* “en” + Token(upperInitial)\n",
        "  * en Murcia, en Orizaba\n",
        "* “en” + “el”|”la”|”los”|”las” + Token\n",
        "  * en el colegio, en la clase, en los botes, en las camas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s044cldTilz-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-D1xs4CIoFQ"
      },
      "source": [
        "## Ejercicio 3 - Para entregar\n",
        "\n",
        "Crear los recursos necesarios para identificar cantidades de dinero en español con los siguientes patrones (reglas):\n",
        "\n",
        "* Token.pos=\"NUM\" + Moneda (euros, dólares)\n",
        "  * 100.000 euros, 200.000 dólares\n",
        "* “\\$” + Token.pos = \"NUM\"\n",
        "  * $ 100.000\n",
        "* Token.pos=\"NUM\" + \"€\"\n",
        "  * 200.000 €\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2xUWjfEItIK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}