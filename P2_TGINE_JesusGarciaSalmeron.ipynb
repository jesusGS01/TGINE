{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO68LN/EFLGOe191C0gzCy2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusGS01/TGINE/blob/main/P2_TGINE_JesusGarciaSalmeron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2 : Análisis de información textual en YouTube\n",
        "\n",
        "Alumno: Jesús García Salmerón\n",
        "\n",
        "Convocatoria: Enero, 2024"
      ],
      "metadata": {
        "id": "bvrHSfdS5YkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Extracción de datos de YouTube"
      ],
      "metadata": {
        "id": "UjYuA04C5psm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-api-python-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nonAEk5v8zqV",
        "outputId": "11cc85c3-7b5b-47f1-ce59-cc484efa76e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.110.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JNdFYZ305XvB"
      },
      "outputs": [],
      "source": [
        "# api_key = \"AIzaSyDy1-AChQ-ywxeJoZeHRdF0zl0I5xZIL88\" # Primera\n",
        "# api_key = \"AIzaSyBf0oNdiVZSoFKm1MmS6mL7k0hY_ZPd6sM\" # Segunda\n",
        "# api_key = \"AIzaSyDnifOG2856kcPlM96kwdvWHzANNRIfHks\" # Tercera\n",
        "api_key = \"AIzaSyBXsFiRKyTdFBSwwac1-U8vPk1bF0WU6F0\" # Cuarta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sacamos los IDs de los canales"
      ],
      "metadata": {
        "id": "-GyW3vH38ryp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "def get_channel_ids(youtube, channel_names, max_results=1):\n",
        "    channel_ids = {}\n",
        "    for channel_name in channel_names:\n",
        "        request = youtube.search().list(\n",
        "            part=\"snippet\",\n",
        "            type=\"channel\",\n",
        "            q=channel_name,\n",
        "            maxResults=max_results\n",
        "        )\n",
        "\n",
        "        response = request.execute()\n",
        "\n",
        "        if 'items' in response and len(response['items']) > 0:\n",
        "            channel_id = response['items'][0]['snippet']['channelId']\n",
        "            channel_ids[channel_name] = channel_id\n",
        "\n",
        "    return channel_ids\n",
        "\n",
        "\n",
        "def save_to_json(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "# Configurar el servicio de YouTube\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "\n",
        "# Crear el cliente de YouTube con la API Key\n",
        "youtube = build(api_service_name, api_version, developerKey=api_key)\n",
        "\n",
        "canales_viaje = [\"Lethal Crysis\",\"Mochileros\",\"Sekaivlog\",\"Molaviajar\",\"Paco Nadal\",\"Oscar Alejandro\",\"Enrique Álex\",\"Luisito Comunica\",\"alanxelmundo\",\"Kike Arnaiz\"]\n",
        "canales_deporte = [\"demas6Basket\",\"ACB\",\"Baloncesto España\", \"Hatch - NBA\", \"Mundo Maldini\",\"TyC Sports\",\"ESPN Deportes\",\"La Media Inglesa\",\"Cracks\",\"LALIGA EA Sports\"]\n",
        "canales_comida = [\"¡Que el papeo te acompañe!\",\"Cocinando con Dario\",\"Lolita la pastelera\",\"PostresSaludables\",\"La Cocina Del Pirata\", \"Diegodoal\", \"GeritaVal\",\"El Mundo En Recetas\",\"Casserola club\", \"Recetas y Más TV\"]\n",
        "\n",
        "# Temática de los canales que quieres buscar\n",
        "temas = [\"Viaje\",\"Deportes\",\"Comida\"]  # Reemplaza con la temática que buscas\n",
        "\n",
        "# Diccionario para almacenar los IDs de los canales por temática\n",
        "canales_por_tematica = {\n",
        "    \"Viaje\": get_channel_ids(youtube, canales_viaje),\n",
        "    \"Deportes\": get_channel_ids(youtube, canales_deporte),\n",
        "    \"Cocina\": get_channel_ids(youtube, canales_comida)\n",
        "}\n",
        "\n",
        "# Guardar los datos en un archivo JSON\n",
        "save_to_json(canales_por_tematica, 'canales_por_tematica.json')\n",
        "print(\"Datos guardados en canales_por_tematica.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N56JFKe78m8O",
        "outputId": "4e560966-f378-444e-c939-299ba45fb903"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos guardados en canales_por_tematica.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sacamos la informacion de cada canal"
      ],
      "metadata": {
        "id": "1rhPTLfF8wA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def get_videos(youtube, channel_id, max_results=100):\n",
        "    request = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        type=\"video\",\n",
        "        channelId=channel_id,\n",
        "        maxResults=max_results\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "    return video_ids\n",
        "\n",
        "\n",
        "def get_video_details(youtube, video_id):\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet\",\n",
        "        id=video_id\n",
        "    )\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    if 'items' in response and len(response['items']) > 0:\n",
        "        video_info = response['items'][0]['snippet']\n",
        "        return {\n",
        "            \"channel\": video_info['channelTitle'],\n",
        "            \"date\": video_info['publishedAt'],\n",
        "            \"title\": video_info['title'],\n",
        "            \"description\": video_info['description']\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_comments(youtube, video_id, max_results=50):\n",
        "    request_comments = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=max_results\n",
        "    )\n",
        "    response_comments = request_comments.execute()\n",
        "\n",
        "    comments = []\n",
        "\n",
        "    for item in response_comments.get('items', []):\n",
        "        comment_data = {\n",
        "            \"user\": item['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
        "            \"comment\": item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
        "            \"sentiment\": \"\"  # Puedes añadir análisis de sentimiento aquí si deseas\n",
        "        }\n",
        "        comments.append(comment_data)\n",
        "\n",
        "    if not comments:\n",
        "        print(f\"No se pudieron encontrar comentarios para el video: {video_id}\")\n",
        "\n",
        "    return comments\n",
        "\n",
        "def read_json(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def save_channel_data(directory, channel_name, tema, data):\n",
        "    # Reemplazar caracteres no válidos en el nombre del canal\n",
        "    invalid_chars = ['/', '\\\\', '?', '%', '*', ':', '|', '\"', '<', '>', '.']\n",
        "    for char in invalid_chars:\n",
        "        channel_name = channel_name.replace(char, '_')\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    file_name = f\"{directory}/{channel_name}.json\"  # Nombre del archivo con el nombre del canal\n",
        "    with open(file_name, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "canales_por_tematica = read_json('canales_por_tematica.json')\n",
        "\n",
        "# Verificar si el directorio existe y eliminarlo si es así\n",
        "if os.path.exists(\"channelsData\"):\n",
        "    shutil.rmtree(\"channelsData\")\n",
        "\n",
        "# Crear el directorio\n",
        "os.makedirs(\"channelsData\")\n",
        "\n",
        "for tema, canales in canales_por_tematica.items():\n",
        "    for canal, canal_id in canales.items():\n",
        "        videos = get_videos(youtube, canal_id)\n",
        "        videos_data = []\n",
        "\n",
        "        print(\"CANAL ACTUAL ->\"+canal)\n",
        "        for video_id in videos:\n",
        "            video_info = get_video_details(youtube, video_id)\n",
        "            if video_info:\n",
        "                comments = get_comments(youtube, video_id)\n",
        "                video_data = {\n",
        "                    \"date\": video_info[\"date\"],\n",
        "                    \"title\": video_info[\"title\"],\n",
        "                    \"description\": video_info[\"description\"],\n",
        "                    \"comments\": comments\n",
        "                }\n",
        "                videos_data.append(video_data)\n",
        "\n",
        "        # Crear la estructura de datos para el archivo JSON por tema y canal\n",
        "        canal_data = {\n",
        "            \"channel\": canal,\n",
        "            \"type\": tema,\n",
        "            \"videos\": videos_data\n",
        "        }\n",
        "\n",
        "        save_channel_data(f\"channelsData/{tema}\", canal, tema, canal_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv0onwKR-gMq",
        "outputId": "15e5f1e2-0d9d-478e-c54f-3e20aee8ed64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CANAL ACTUAL ->Lethal Crysis\n",
            "CANAL ACTUAL ->Mochileros\n",
            "No se pudieron encontrar comentarios para el video: Bawf-_rMLxc\n",
            "No se pudieron encontrar comentarios para el video: 7YbNUke0xBQ\n",
            "No se pudieron encontrar comentarios para el video: L0hj7xQK49c\n",
            "CANAL ACTUAL ->Sekaivlog\n",
            "CANAL ACTUAL ->Molaviajar\n",
            "CANAL ACTUAL ->Paco Nadal\n",
            "CANAL ACTUAL ->Oscar Alejandro\n",
            "CANAL ACTUAL ->Enrique Álex\n",
            "CANAL ACTUAL ->Luisito Comunica\n",
            "CANAL ACTUAL ->alanxelmundo\n",
            "CANAL ACTUAL ->Kike Arnaiz\n",
            "CANAL ACTUAL ->demas6Basket\n",
            "CANAL ACTUAL ->ACB\n",
            "No se pudieron encontrar comentarios para el video: 2D2vBZCpfAI\n",
            "No se pudieron encontrar comentarios para el video: HI8nYtLaZlI\n",
            "No se pudieron encontrar comentarios para el video: 1wuuBZ1_zoo\n",
            "No se pudieron encontrar comentarios para el video: cqZ8sRCj0Yc\n",
            "No se pudieron encontrar comentarios para el video: dmXs-8NlSSE\n",
            "No se pudieron encontrar comentarios para el video: pjsmFAQc7eI\n",
            "No se pudieron encontrar comentarios para el video: uBakR1LaseE\n",
            "No se pudieron encontrar comentarios para el video: yOV7YxSrVdo\n",
            "CANAL ACTUAL ->Baloncesto España\n",
            "No se pudieron encontrar comentarios para el video: tEz0SElJ7ho\n",
            "No se pudieron encontrar comentarios para el video: AgoFLXJi4i0\n",
            "No se pudieron encontrar comentarios para el video: v_9ofqJU3fE\n",
            "No se pudieron encontrar comentarios para el video: MnKLs0IFIrA\n",
            "No se pudieron encontrar comentarios para el video: 1OcyAutpA3s\n",
            "No se pudieron encontrar comentarios para el video: ikez4wrgais\n",
            "No se pudieron encontrar comentarios para el video: XlnWfuWpjoI\n",
            "No se pudieron encontrar comentarios para el video: zZ59XwdjrQs\n",
            "No se pudieron encontrar comentarios para el video: l3lFfHzu2DA\n",
            "No se pudieron encontrar comentarios para el video: pqJlX5RFbsU\n",
            "CANAL ACTUAL ->Hatch - NBA\n",
            "CANAL ACTUAL ->Mundo Maldini\n",
            "CANAL ACTUAL ->TyC Sports\n",
            "CANAL ACTUAL ->ESPN Deportes\n",
            "CANAL ACTUAL ->La Media Inglesa\n",
            "CANAL ACTUAL ->Cracks\n",
            "CANAL ACTUAL ->LALIGA EA Sports\n",
            "CANAL ACTUAL ->¡Que el papeo te acompañe!\n",
            "CANAL ACTUAL ->Cocinando con Dario\n",
            "No se pudieron encontrar comentarios para el video: npZNeiiYxb8\n",
            "No se pudieron encontrar comentarios para el video: qXrcmXAS_Ko\n",
            "CANAL ACTUAL ->Lolita la pastelera\n",
            "CANAL ACTUAL ->PostresSaludables\n",
            "CANAL ACTUAL ->La Cocina Del Pirata\n",
            "CANAL ACTUAL ->Diegodoal\n",
            "CANAL ACTUAL ->GeritaVal\n",
            "No se pudieron encontrar comentarios para el video: EXsf52GFfOk\n",
            "No se pudieron encontrar comentarios para el video: dBYML2V0PiQ\n",
            "No se pudieron encontrar comentarios para el video: LEUIUTvorhQ\n",
            "No se pudieron encontrar comentarios para el video: 4zexggKDRm4\n",
            "No se pudieron encontrar comentarios para el video: 4XcJ846_Nkw\n",
            "No se pudieron encontrar comentarios para el video: UA_mApDBpRA\n",
            "No se pudieron encontrar comentarios para el video: y0ou62_wM1Q\n",
            "No se pudieron encontrar comentarios para el video: Xjv1479Cbos\n",
            "No se pudieron encontrar comentarios para el video: -mpVc3vsEBY\n",
            "No se pudieron encontrar comentarios para el video: t8UQuwScv9k\n",
            "No se pudieron encontrar comentarios para el video: YefP06rQNqA\n",
            "No se pudieron encontrar comentarios para el video: tmwbBWgMxdw\n",
            "No se pudieron encontrar comentarios para el video: Hdj01vnp3ok\n",
            "No se pudieron encontrar comentarios para el video: bxXQumRbYPE\n",
            "No se pudieron encontrar comentarios para el video: V0Rv6HyA8x4\n",
            "No se pudieron encontrar comentarios para el video: 9lhac1MwAqY\n",
            "No se pudieron encontrar comentarios para el video: DrqsYpTlTnA\n",
            "No se pudieron encontrar comentarios para el video: otuZ1-xPs7w\n",
            "No se pudieron encontrar comentarios para el video: oebpBJ-Z85U\n",
            "No se pudieron encontrar comentarios para el video: LY1EX235ZfM\n",
            "No se pudieron encontrar comentarios para el video: ujLUKi7CPAo\n",
            "No se pudieron encontrar comentarios para el video: WpI_f5kqksw\n",
            "No se pudieron encontrar comentarios para el video: CypkYgIEyYs\n",
            "No se pudieron encontrar comentarios para el video: Y1LLluoAr-c\n",
            "No se pudieron encontrar comentarios para el video: 2SP13-SqxGg\n",
            "No se pudieron encontrar comentarios para el video: VSQStcBaSV8\n",
            "No se pudieron encontrar comentarios para el video: YptDoDHsgsk\n",
            "No se pudieron encontrar comentarios para el video: ka3zVQtn9n4\n",
            "No se pudieron encontrar comentarios para el video: J6M9ysIAgdk\n",
            "CANAL ACTUAL ->El Mundo En Recetas\n",
            "No se pudieron encontrar comentarios para el video: dmrVk-WoIcw\n",
            "No se pudieron encontrar comentarios para el video: lAoJ63IGgqA\n",
            "CANAL ACTUAL ->Casserola club\n",
            "No se pudieron encontrar comentarios para el video: nLlr9jok8yM\n",
            "CANAL ACTUAL ->Recetas y Más TV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 - Clasificador del tipo de canal"
      ],
      "metadata": {
        "id": "fGX8LS7dVlEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datos de Train y Validation"
      ],
      "metadata": {
        "id": "d-EMXmZ5gxJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Train\n",
        "train_data = []\n",
        "train_labels = []\n",
        "# Validation\n",
        "val_data = []\n",
        "val_labels = []\n",
        "\n",
        "# Directorio padre\n",
        "directory = 'channelsData'\n",
        "\n",
        "# Contadores para saber el numero de canales\n",
        "count_cocina = 0\n",
        "count_deportes = 0\n",
        "count_viaje = 0\n",
        "\n",
        "# Recorrer los directorios en channelsData\n",
        "for root, dirs, files in os.walk(directory):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            data_type = data['type']\n",
        "            if data_type == \"Cocina\":\n",
        "              count_cocina += 1\n",
        "              actual_type = count_cocina\n",
        "            elif data_type == \"Deportes\":\n",
        "              count_deportes += 1\n",
        "              actual_type = count_deportes\n",
        "            elif data_type == \"Viaje\":\n",
        "              count_viaje += 1\n",
        "              actual_type = count_viaje\n",
        "\n",
        "            data_videos = data['videos']\n",
        "            for video in data_videos:\n",
        "              if actual_type <= 7:\n",
        "                train_data.append(video['description'])\n",
        "                train_labels.append(data['type'])\n",
        "              else:\n",
        "                val_data.append(video['description'])\n",
        "                val_labels.append(data['type'])\n",
        "\n",
        "\n",
        "# Verificamos el tamaño de los conjuntos de entrenamiento y validación\n",
        "print(f\"Tamaño de datos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Tamaño de datos de validación: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDvurUmTg0DU",
        "outputId": "ad7c08d0-f5c3-455a-92f5-33bd71b80ede"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de datos de entrenamiento: 1049\n",
            "Tamaño de datos de validación: 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos a los distintos clasificadores"
      ],
      "metadata": {
        "id": "0TVhBooZqWg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Creamos el pipeline de TF con LinearSVC\n",
        "clf_tf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tf', TfidfTransformer(use_idf=False)),\n",
        "    ('clf', LinearSVC(random_state=0, tol=1e-5)),])\n",
        "\n",
        "# Creamos el pipeline de TFIDF con LinearSVC y lo guardamos en clf_tfidf\n",
        "clf_tfidf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC(random_state=0, tol=1e-5)),])\n",
        "\n",
        "# Entrenamos el modelo de TF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tf.fit(train_data, train_labels)\n",
        "\n",
        "# Entrenamos el modelo de TFIDF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tfidf.fit(train_data, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "LuADynXMnyUt",
        "outputId": "2889ea01-7899-44bf-d02b-759eb4faf9ec"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', LinearSVC(random_state=0, tol=1e-05))])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;clf&#x27;, LinearSVC(random_state=0, tol=1e-05))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;clf&#x27;, LinearSVC(random_state=0, tol=1e-05))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(random_state=0, tol=1e-05)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Creamos el pipeline de TF con RandomForestClassifier\n",
        "clf_tf_rf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tf', TfidfTransformer(use_idf=False)),\n",
        "    ('clf', RandomForestClassifier(random_state=0)),])\n",
        "\n",
        "# Creamos el pipeline de TFIDF con RandomForestClassifier y lo guardamos en clf_tfidf_rf\n",
        "clf_tfidf_rf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', RandomForestClassifier(random_state=0)),])\n",
        "\n",
        "# Entrenamos el modelo de TF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tf_rf.fit(train_data, train_labels)\n",
        "\n",
        "# Entrenamos el modelo de TFIDF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tfidf_rf.fit(train_data, train_labels)"
      ],
      "metadata": {
        "id": "nK7hmNiipZU-",
        "outputId": "2fced020-6e60-43af-e7dd-69b31bc22a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', RandomForestClassifier(random_state=0))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;clf&#x27;, RandomForestClassifier(random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
              "                (&#x27;clf&#x27;, RandomForestClassifier(random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Creamos el pipeline de TF con Gradient Boosting\n",
        "clf_tf_gb = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tf', TfidfTransformer(use_idf=False)),\n",
        "    ('clf', GradientBoostingClassifier(random_state=0)),])\n",
        "\n",
        "# Creamos el pipeline de TFIDF con Gradient Boosting\n",
        "clf_tfidf_gb = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', GradientBoostingClassifier(random_state=0)),])\n",
        "\n",
        "# Entrenamos el modelo de TF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tf_gb.fit(train_data, train_labels)\n",
        "\n",
        "# Entrenamos el modelo de TFIDF con el conjunto de entrenamiento con sus etiquetas\n",
        "clf_tfidf_gb.fit(train_data, train_labels)"
      ],
      "metadata": {
        "id": "ZHhq64hbqZ7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos resultados de validacion"
      ],
      "metadata": {
        "id": "-UwU9jYFqTym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Evaluamos el TF\n",
        "predicted_tf = clf_tf.predict(val_data)\n",
        "accuracy_tf = np.mean(predicted_tf == val_labels)\n",
        "\n",
        "print(\"Resultados TF ----- Accuracy:\", accuracy_tf)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(val_labels, predicted_tf))\n",
        "\n",
        "# Evaluamos el TFIDF\n",
        "predicted_tfidf = clf_tfidf.predict(val_data)\n",
        "accuracy_tfidf = np.mean(predicted_tfidf == val_labels)\n",
        "\n",
        "print(\"Resultados TFIDF ----- Accuracy:\", accuracy_tfidf)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(val_labels, predicted_tfidf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EqUwv4smDec",
        "outputId": "c3c8ca2e-19af-4029-cfad-9fdf1694c2c7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados TF ----- Accuracy: 0.5933333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Cocina       0.60      0.18      0.28       150\n",
            "    Deportes       0.60      0.83      0.70       150\n",
            "       Viaje       0.59      0.77      0.66       150\n",
            "\n",
            "    accuracy                           0.59       450\n",
            "   macro avg       0.59      0.59      0.55       450\n",
            "weighted avg       0.59      0.59      0.55       450\n",
            "\n",
            "Resultados TFIDF ----- Accuracy: 0.6622222222222223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Cocina       0.87      0.27      0.41       150\n",
            "    Deportes       0.66      0.88      0.75       150\n",
            "       Viaje       0.62      0.84      0.71       150\n",
            "\n",
            "    accuracy                           0.66       450\n",
            "   macro avg       0.72      0.66      0.62       450\n",
            "weighted avg       0.72      0.66      0.62       450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el TF\n",
        "predicted_tf = clf_tf_rf.predict(val_data)\n",
        "accuracy_tf = np.mean(predicted_tf == val_labels)\n",
        "\n",
        "print(\"Resultados TF ----- Accuracy:\", accuracy_tf)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(val_labels, predicted_tf))\n",
        "\n",
        "# Evaluamos el TFIDF\n",
        "predicted_tfidf = clf_tfidf_rf.predict(val_data)\n",
        "accuracy_tfidf = np.mean(predicted_tfidf == val_labels)\n",
        "\n",
        "print(\"Resultados TFIDF ----- Accuracy:\", accuracy_tfidf)\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(val_labels, predicted_tfidf))"
      ],
      "metadata": {
        "id": "u5pORZGbpdk1",
        "outputId": "a7e52017-080a-4d91-c6ac-bbfcfe29b936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados TF ----- Accuracy: 0.64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Cocina       0.88      0.49      0.63       150\n",
            "    Deportes       0.52      0.83      0.64       150\n",
            "       Viaje       0.71      0.60      0.65       150\n",
            "\n",
            "    accuracy                           0.64       450\n",
            "   macro avg       0.70      0.64      0.64       450\n",
            "weighted avg       0.70      0.64      0.64       450\n",
            "\n",
            "Resultados TFIDF ----- Accuracy: 0.6444444444444445\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Cocina       0.87      0.51      0.64       150\n",
            "    Deportes       0.51      0.83      0.64       150\n",
            "       Viaje       0.74      0.59      0.66       150\n",
            "\n",
            "    accuracy                           0.64       450\n",
            "   macro avg       0.71      0.64      0.65       450\n",
            "weighted avg       0.71      0.64      0.65       450\n",
            "\n"
          ]
        }
      ]
    }
  ]
}